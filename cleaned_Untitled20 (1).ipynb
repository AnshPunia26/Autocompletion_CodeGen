{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "zSBCTVXjA7cD",
    "outputId": "bddd2812-f4e6-42b9-a1df-b90b1de86022"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hLooking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.14.4\n",
      "    Uninstalling datasets-2.14.4:\n",
      "      Successfully uninstalled datasets-2.14.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.6.0 fsspec-2025.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -q bitsandbytes accelerate peft\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install --upgrade datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6HU0MRo1BKOW"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "login(hf_token, add_to_git_credential=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r_nzYnO7BKLp",
    "outputId": "954de209-7029-4c39-b0b5-f524a1f8f17e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()  # Clear unused GPU memory\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120,
     "referenced_widgets": [
      "1418c95169d44696a4af3d9f8e2c7b65",
      "48dd9ad4998f4b3a861368c7b29213b2",
      "b464dd05699440bd9f740842634505bf",
      "b3d6fdfd54404c13b696cc77b641ec7d",
      "88af211813c7448f8f7e82891edc1df2",
      "3d3d3cc2d928439f935191b4a50990db",
      "74c564d9f9d54d9f82a99bd1728caecd",
      "f96e004764464298989c8f3cad551cd1",
      "33d5da25b5a843f58c0e3e63b08ab2f4",
      "fe81c7b4bbaf4610af44124384aa65e2",
      "b97848184210480bb57983019c918584"
     ]
    },
    "collapsed": true,
    "id": "Y42R6n5MBKHH",
    "outputId": "35cb22de-3087-49d2-aa48-805867b3f391"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1418c95169d44696a4af3d9f8e2c7b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.71k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IterableDataset({\n",
      "    features: ['hexsha', 'size', 'content', 'avg_line_length', 'max_line_length', 'alphanum_fraction'],\n",
      "    num_shards: 8\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"ammarnasr/the-stack-java-clean\", split='train', streaming=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "nQxCD7q-BKBG",
    "outputId": "1b6d61fa-c180-4c2a-c386-484aef0f2ca2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffered data was truncated after reaching the output size limit."
     ]
    }
   ],
   "source": [
    "# Collect code samples\n",
    "code_samples = []\n",
    "for i, sample in enumerate(dataset):\n",
    "    code_samples.append(sample['content'])\n",
    "    if i == 15000:\n",
    "        break\n",
    "\n",
    "for idx, code in enumerate(code_samples):\n",
    "    print(f\"Code Sample {idx + 1}:\\n{code}\\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CtkXTpGwCG8F"
   },
   "outputs": [],
   "source": [
    "training_data = [{\"prompt\": code, \"completion\": code} for code in code_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X7TuuzX7CmFG",
    "outputId": "6eef394b-b2cc-4366-b9df-0b440980a65b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 15001\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training data size: {len(training_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209,
     "referenced_widgets": [
      "6a4df412ffd240a281dc5afbb6d8da0d",
      "40b7d35a06794e5a85bda22ca3e82f12",
      "fbb20b22f5c34b2eacfa5998933ef7f0",
      "d3bfcd41b32c4502a3d3eba9a201d5e2",
      "8f244806de4f4543b952722e5c0d08d4",
      "111abc065b224d3d9bb35a15596d7e50",
      "050720112b584af681eb01522a3396b8",
      "08503250db444dac82a54a4c0504990d",
      "93ebf9da8e3644f5b489162b20202011",
      "6876cea02093490eb81f39d18a9de9fa",
      "54623c85a48649c683fd44cb8c792980",
      "a4f82dabe44146af8a1108352cca9132",
      "4a38564cd6fc4faeb3c1572fb3bab23e",
      "a89f8958a2534385abb5e27d97362821",
      "052e4e386b0c41f19c57e393ed7d79a2",
      "ffa18ab9d4124510b898cc80aec5ed28",
      "62e6587ceff8496c9631262da58b95d0",
      "fcab4e728eb9421ba38543750ea8d206",
      "2a2cdc8324534d1a832df3e81c4cb166",
      "3f606a6cad904513aed4015703bd5562",
      "4993f8f51bf64a95973986ec1b91b188",
      "85378bc90faa4c3eb187c4a7c45bf00f",
      "6e28f89c7b864c69a57f1c658dacb775",
      "063129432e6945d0bfd9b8435d170819",
      "d6a5f9545ddf4420989343ed5fc997ac",
      "fa23c9c39e0a4a65b4aaba3c6c525d48",
      "88f4cad5a7ce4504b1ce1e6f145c2ecc",
      "7e2bfee0b29e44f48c935c6cfd4f5aa4",
      "2bec829e30914986b988f747218e6086",
      "f6a8826ba05a4d699a0fa9e73ed2644b",
      "cec4bc161a18448c887ddc2f4ff83472",
      "157f1ba2d5d142e09b3a59de730ab91d",
      "dc6d5fce7f684d1da45e07c20ed994a3",
      "92083acf43914fbfad2e1b6b037c3cd7",
      "40e5118922b04c3d97c68e0fe15d5960",
      "35653615f34d4e4f8a1890c84a1d02ef",
      "36ae1c4ac13d4618b07348e096b86f5c",
      "33b0795aa76e47bf97255032e8d7cdd4",
      "115e53624f0c4f5bacc0148dc0c92fc1",
      "6741305e119b4340849183c7a5fca5ea",
      "8412cfad7dac4661a3bf5b9f9788f16a",
      "21282e844cde4dc5b9d15d28858af90f",
      "57e9f960408d4e7ebf4acff1821296c3",
      "c9c97f6c9cd94d6f82031cf40caad2de",
      "94581766807340f0911d043e6ae46075",
      "9e162841791741d0820350c504d1702b",
      "7ac1c1cfe76c4340861bdf4ee8f3ed23",
      "bdf1f2d04c164871b2c886524db1b83d",
      "47d4610863b8480da70c8bdae9b46a33",
      "6d335a2fba234bcbaa063342d439421b",
      "6b512810933942128eda38c3c260141a",
      "a81a124a36934c27ab575453501bbb68",
      "7e88a74abc174f919a86db08c5301456",
      "9f1c4ad98133474eb18a7150a536e2fd",
      "7d84f707d51249418e04f16ee8d8b3ab",
      "20d10a4f69ac41f6b3ee65ac763afce3",
      "de44cb35cbb944998a48cc95c99c7977",
      "17958219c14249c0b20de1a0c98c814e",
      "fbb8d69b767c4f7e9e8c9f4af37a0be0",
      "40231f2a07a343a3b6b5d21b20507509",
      "b7f24a3ad26b463fae48af8bad59d28f",
      "7927e690b97f47b490ac4db9955618f0",
      "744d26eef20546a289e89f5e5db98c21",
      "795c2b6085624f0ebe554cfbb93346f9",
      "0d821d9ca7bb4d639da122fe79032636",
      "072794ff77cb4947b49d33ce9dbe59f3"
     ]
    },
    "id": "VCI2GAMlCopw",
    "outputId": "2e3ee788-9dc4-4e0b-dcd6-1289f0e1434f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a4df412ffd240a281dc5afbb6d8da0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f82dabe44146af8a1108352cca9132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e28f89c7b864c69a57f1c658dacb775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92083acf43914fbfad2e1b6b037c3cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94581766807340f0911d043e6ae46075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d10a4f69ac41f6b3ee65ac763afce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")\n",
    "\n",
    "text = training_data[0]['prompt']\n",
    "inputs = tokenizer(\n",
    "    text,\n",
    "    max_length=512,  # Reduce sequence length\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=True, # Add special tokens\n",
    ")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbMcUaVZCssU"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241,
     "referenced_widgets": [
      "e6fd7d3796bd494394942a715c224ca4",
      "dca55f759b5a46c1951f71d5e4d1374d",
      "590f2bc631114a32ba5d6d0908a0d54d",
      "18ea46ada0c04d858975edc8a54a6018",
      "246fa63754b24250a40ef0ff3b2c264f",
      "1dd233bcbd9c44e8bc131554007e720c",
      "ccb9e05e52b54e3b992958123c322701",
      "54aa2f21a04a4fdeb1b042b78e40edc2",
      "d46814b399da4303a81bfefcc6b88363",
      "709e1be89dd142c589bd5f2f1207490b",
      "9836026a6c7c42f492f3c1e5e6344daf",
      "52557a2069e64b9a847b1500cf3b085d",
      "387bb5c40aea47198d9f44aeb072007e",
      "e57fe0aaa7e1482eaa4b9559ef31c29a",
      "eee9f1a788014a76b2ea73e839a85882",
      "f841ab102ae64a569e06ff440631b4d5",
      "dab9f4c32c44489194668b46f7de8a7c",
      "355e44a846b44b3bb16d1bad0c49f125",
      "e8b759de74924679b77e7c21d0ed1b85",
      "b4c7c17578264073871858b5d3f23092",
      "e654925c309d4351b8f587c97c51cc2c",
      "296af1874ccd4613baf5013629af619c",
      "0575eee9dcb54ae3898496fe5e8f70e8",
      "9c863f81ff8e4f0ab9cb8373a9a976c2",
      "2ebf0b4f0ab040bdb26c2963bd631346",
      "37a7a968050244fab6b180fc231b4b22",
      "a5946b59dd834bd19f63710af162e9bb",
      "8d81aa55af6943ff9de0b1c30c41c560",
      "331b3e8262654218bcd8aa294f9495bc",
      "a790c50a6e67403a9a341c73550142ef",
      "6487e64a55f4467cae9de5c606b7542a",
      "b835f592d422404999db2917d0c04dce",
      "05caed833d344a73a607873c207777f1",
      "ac28d7f069574be084415a40511cfa5c",
      "f664f37f65724eee9feb8e6e4b7f4070",
      "6aafc93f97b9425881cbc0858ad7522b",
      "4f0f6e6e07144f888358f9ad9b6a2f4e",
      "21f36050c6a54c9aa028c8ed78431150",
      "d9428c83fe2f44889573d6c6d6ae8096",
      "315db0a60bcf4a929c381fb18c738a43",
      "66223c317e7544d4ba47cbe84487481c",
      "3ac24c6668f74595b966a84dcd9dbfd9",
      "fa75518d75d2404d88f437ae24a1300b",
      "7a7ff7a54cde4896a1815614f3652798",
      "d8f7cc00392b40b296c1febd149dd853",
      "b5891463996d4995bd5df53d271b545c",
      "5aa80ea08e8045d5a2a32a9554a73a3a",
      "40b0f082bb61462380fe6a4688e26dc9",
      "bcd2f221f38b4bf2a558ef5db4daa25c",
      "4ac3b1273e704702a78e112c6eadbd67",
      "750822c747ea4b5d9ea111a7dfd35a98",
      "085b5195793e4ffc9a6821e3ca81a704",
      "eff6362cd5944fe6a60e60dae7ff0d15",
      "a99f3056ae07461dba6f88fb9d35a283",
      "b48be627802e415a88d2af98b63adad2",
      "2f8287a4932f4c63b51b3d6b617e9530",
      "e6441d966a6849d9a938f8e72a3b6c5b",
      "c28a8ceccf63467dab0ad1d9780e2c3c",
      "9a9eb6dfa1784b69b4d3bcbf12080290",
      "589eda1fd981480bbccd8b62e89d4dfe",
      "0dc3e915695e45f58265f6b649bea8b9",
      "163c20c9afaf48eb9c4863a52ec9b1b2",
      "16e4799c449f46fd8cdf847ff9562e82",
      "85f6841c2898469da524a6532dfa4f4a",
      "fed4284cd05845ba90226ed939bba2de",
      "c2f5790f0cf8451a868d9080e8ba76c0",
      "2da2144eadee47888c2a187b4b9d3d47",
      "4b3e5115ccb64cca8adbdb78246307ea",
      "048008851ef54e678a8173448bc1f366",
      "f983f5f9fb4e43fb939043fff9ddef52",
      "be25a86e049c46299eebdd1bf37334ff",
      "232e11f2711440328451a6fdb1d84def",
      "758bf7815cca48d993d5c8b324ea43c5",
      "9953f02d765a46aa8aa880b8fd74ade5",
      "75c7ee0e9c75438e886cbcac85a015c8",
      "104c289ebb4e45a3a71960495663ce8b",
      "d03954e1a9994fc5812b8bca317df0ab"
     ]
    },
    "id": "ir3c4vAHCxV9",
    "outputId": "211ead43-a46c-4925-e958-c1f7c7b84a25"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6fd7d3796bd494394942a715c224ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52557a2069e64b9a847b1500cf3b085d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/35.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0575eee9dcb54ae3898496fe5e8f70e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac28d7f069574be084415a40511cfa5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f7cc00392b40b296c1febd149dd853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f8287a4932f4c63b51b3d6b617e9530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da2144eadee47888c2a187b4b9d3d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/phi-2\",\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    ),\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cm3G-bYCxS1"
   },
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4QvrTFY9CxQG",
    "outputId": "ecb7fed8-bcff-4e9d-952b-23feaef93ce9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiForCausalLM(\n",
       "  (model): PhiModel(\n",
       "    (embed_tokens): Embedding(51200, 2560)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x PhiDecoderLayer(\n",
       "        (self_attn): PhiAttention(\n",
       "          (q_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
       "          (dense): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
       "        )\n",
       "        (mlp): PhiMLP(\n",
       "          (activation_fn): NewGELUActivation()\n",
       "          (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (rotary_emb): PhiRotaryEmbedding()\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vV37gxTCDUKk"
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oAaqADnhDWxG",
    "outputId": "36b0e801-4c3e-4803-e06f-4753b8b277a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): PhiForCausalLM(\n",
       "      (model): PhiModel(\n",
       "        (embed_tokens): Embedding(51200, 2560)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x PhiDecoderLayer(\n",
       "            (self_attn): PhiAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (dense): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
       "            )\n",
       "            (mlp): PhiMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n",
       "              (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n",
       "            )\n",
       "            (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (rotary_emb): PhiRotaryEmbedding()\n",
       "        (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CGY7z7VqDZC3"
   },
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model = model.half().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n6Xqql9_DdOh"
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "inputs = {\n",
    "    k: v.to(device) if v.dtype == torch.float else v.to(device)\n",
    "    for k, v in inputs.items()\n",
    "}\n",
    " # Move + convert to fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qO0dFVlRDfa6",
    "outputId": "ce604949-d187-4375-bbf6-b16ec0cccea9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample loss: 1.138396143913269\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "labels = inputs['input_ids'].clone()\n",
    "outputs = model(**inputs, labels=labels)\n",
    "loss = outputs.loss\n",
    "print(f\"Sample loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ljdNq8e3Dhm3",
    "outputId": "e536728b-0568-4af6-9883-98479143b73d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,621,440 || all params: 2,782,305,280 || trainable%: 0.0942\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwNgVTalDkbj"
   },
   "outputs": [],
   "source": [
    "test_prompt = \"\"\" class DynamicProgramming {\n",
    "    static int knapsack(int[] wt, int[] val, int n, int W) {\n",
    "        int dp[][] = new int[n][W + 1];\n",
    "\n",
    "        for (int i = wt[0]; i <= W; i++) {\n",
    "            dp[0][i] = val[0];\n",
    "        }\n",
    "\n",
    "        for (int ind = 1; ind < n; ind++) {\n",
    "            for (int cap = 0; cap <= W; cap++) {\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0mUMy5iZDnTu"
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tSq6a0H2Doz4"
   },
   "outputs": [],
   "source": [
    "test_inputs = tokenizer(\n",
    "    test_prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    padding=True\n",
    ").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HzAcTMY1Dqpp",
    "outputId": "a5b853ea-e37d-4abc-a5a5-76cb16454dcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(50284, device='cuda:0') 50257\n"
     ]
    }
   ],
   "source": [
    "print(test_inputs['input_ids'].max(), tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G9Ayz0PQDubu"
   },
   "outputs": [],
   "source": [
    "generated_ids = model.generate(\n",
    "    input_ids=test_inputs['input_ids'],\n",
    "    attention_mask=test_inputs['attention_mask'],\n",
    "    max_new_tokens=450,\n",
    "    do_sample=True,\n",
    "    temperature=0.3,\n",
    "    top_p=0.95,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uc0oChGCDxQX",
    "outputId": "212fc31f-abb1-4adb-b677-b5edefc4cc66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " class DynamicProgramming {\n",
      "    static int knapsack(int[] wt, int[] val, int n, int W) {\n",
      "        int dp[][] = new int[n][W + 1];\n",
      "        \n",
      "        for (int i = wt[0]; i <= W; i++) {\n",
      "            dp[0][i] = val[0];\n",
      "        }\n",
      "        \n",
      "        for (int ind = 1; ind < n; ind++) {\n",
      "            for (int cap = 0; cap <= W; cap++) {\n",
      "                if (cap >= wt[ind]) {\n",
      "                    dp[ind][cap] = Math.max(val[ind] + dp[ind - 1][cap - wt[ind]], dp[ind - 1][cap]);\n",
      "                } else {\n",
      "                    dp[ind][cap] = dp[ind - 1][cap];\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "        \n",
      "        return dp[n - 1][W];\n",
      "    }\n",
      "    \n",
      "    public static void main(String[] args) {\n",
      "        int n = 4;\n",
      "        int[] wt = {4, 3, 2, 2};\n",
      "        int[] val = {2, 3, 5, 7};\n",
      "        System.out.println(knapsack(wt, val, n, 5));\n",
      "    }\n",
      "}\n",
      "\n",
      "A:\n",
      "\n",
      "First, there is a bug in your code:\n",
      "dp[ind][cap] = Math.max(val[ind] + dp[ind - 1][cap - wt[ind]], dp[ind - 1][cap]);\n",
      "\n",
      "For some values of ind and cap, val[ind] is 0 (or negative) and Math.max(0, dp[ind - 1][cap - wt[ind])) will return 0, which is incorrect.\n",
      "There are a few ways to fix this. One is to change val[ind] to 1 if it is zero, but that won't work for negative values. Another is to use Math.max(val[ind], 0) instead.\n",
      "The bug in your code is a result of trying to use dp[ind - 1][cap - wt[ind]], but dp[ind - 1] does not exist when ind == 0. This is why you get an ArrayIndexOutOfBoundsException.\n",
      "You can fix this by changing your loop to start with ind == 0 and add an if statement to check if cap is greater than or equal to 0 before trying to access dp[ind - 1][cap - wt[ind]].\n",
      "\n",
      "A:\n",
      "\n",
      "I think you can use this code to get the answer.\n",
      "    public static int knapsack(int[] wt, int[] val, int n, int W) {\n",
      "        int dp[][] = new int[n][W + 1];\n",
      "\n",
      "        for (int i\n"
     ]
    }
   ],
   "source": [
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "AQ2wISlGbbd4"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}